{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmriAbdalla/computingCloudCourse/blob/main/cloud_tut6_firstWorkWithSearchEngine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voQt1cu0ce-h",
        "outputId": "5d1e9467-acd6-439f-b28a-1b19be2a9566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n"
          ]
        }
      ],
      "source": [
        "# Install the required libraries: 'requests' for HTTP requests, 'beautifulsoup4' for parsing HTML\n",
        "!pip install requests beautifulsoup4\n",
        "\n",
        "# Import the necessary modules\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Function to fetch and parse the content of a web page\n",
        "def fetch_page(url):\n",
        "    # Send a GET request to the specified URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # If the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content of the page using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        return soup\n",
        "    else:\n",
        "        # If the request failed, return None\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to index and count all words in the text extracted from a BeautifulSoup object\n",
        "def index_words(soup):\n",
        "    index = {}  # Dictionary to store word frequencies\n",
        "\n",
        "    # Extract all words using regex (only alphanumeric words) from the visible text of the page\n",
        "    words = re.findall(r'\\w+', soup.get_text())\n",
        "\n",
        "    # Loop through each word\n",
        "    for word in words:\n",
        "        word = word.lower()  # Convert word to lowercase for uniformity\n",
        "        if word in index:\n",
        "            index[word] += 1  # If the word already exists, increment its count\n",
        "        else:\n",
        "            index[word] = 1   # Otherwise, add it to the dictionary with count 1\n",
        "\n",
        "    return index  # Return the dictionary of word counts\n"
      ],
      "metadata": {
        "id": "z_oG5HYzc0dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(index):\n",
        "    \"\"\"\n",
        "    Removes common stop words from a word frequency dictionary.\n",
        "\n",
        "    Parameters:\n",
        "        index (dict): A dictionary where keys are words (str) and values are their frequency counts (int).\n",
        "\n",
        "    Returns:\n",
        "        dict: The same dictionary with specified stop words removed.\n",
        "    \"\"\"\n",
        "    stop_words = {'a', 'an', 'the', 'and', 'or', 'in', 'on', 'at'}\n",
        "    for stop_word in stop_words:\n",
        "        if stop_word in index:\n",
        "            del index[stop_word]\n",
        "    return index\n"
      ],
      "metadata": {
        "id": "6Qb9vL0rdPyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def apply_stemming(index):\n",
        "    \"\"\"\n",
        "    Applies stemming to all words in a word frequency dictionary using the Porter stemming algorithm.\n",
        "\n",
        "    Parameters:\n",
        "        index (dict): A dictionary where keys are words (str) and values are their frequency counts (int).\n",
        "\n",
        "    Returns:\n",
        "        dict: A new dictionary where words have been reduced to their stems,\n",
        "              combining the counts of words with the same stem.\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_index = {}\n",
        "    for word, count in index.items():\n",
        "        stemmed_word = stemmer.stem(word)\n",
        "        if stemmed_word in stemmed_index:\n",
        "            stemmed_index[stemmed_word] += count\n",
        "        else:\n",
        "            stemmed_index[stemmed_word] = count\n",
        "    return stemmed_index\n"
      ],
      "metadata": {
        "id": "hKG3m2TOdYTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query, index):\n",
        "  \"\"\"\n",
        "    Searches for words from the query in the provided index.\n",
        "\n",
        "    Parameters:\n",
        "    - query (str): The search query string containing one or more words.\n",
        "    - index (dict): A dictionary where keys are words and values are their frequencies.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary containing only the words from the query that are found in the index,\n",
        "            along with their corresponding frequencies.\n",
        "    \"\"\"\n",
        "  query_words = re.findall(r'\\w+', query.lower())\n",
        "  results = {}\n",
        "  for word in query_words:\n",
        "    if word in index:\n",
        "      results[word] = index[word]\n",
        "  return results"
      ],
      "metadata": {
        "id": "qYFBlAu5dpQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_engine(url, query):\n",
        "  \"\"\"\n",
        "    A simple search engine that retrieves and processes text from a web page,\n",
        "    and searches for a query within the processed content.\n",
        "\n",
        "    Steps:\n",
        "    1. Fetches the web page content using the provided URL.\n",
        "    2. Extracts and indexes the words from the page.\n",
        "    3. Removes common stop words.\n",
        "    4. Applies stemming to normalize word forms.\n",
        "    5. Searches for the query terms in the processed index.\n",
        "\n",
        "    Parameters:\n",
        "    - url (str): The URL of the web page to fetch and analyze.\n",
        "    - query (str): The search query string.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary of the query terms found in the web page and their frequencies.\n",
        "            Returns None if the page could not be fetched.\n",
        "    \"\"\"\n",
        "  soup = fetch_page(url)\n",
        "  if soup is None:\n",
        "    return None\n",
        "  index = index_words(soup)\n",
        "  index = remove_stop_words(index)\n",
        "  index = apply_stemming(index)\n",
        "  results = search(query, index)\n",
        "  return results"
      ],
      "metadata": {
        "id": "UJ7bA3_bd302"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the URL of the web page to scrape\n",
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "\n",
        "# Define the search query\n",
        "query = 'birds wings'\n",
        "\n",
        "# Run the custom search engine on the given URL and query\n",
        "results = search_engine(url, query)\n",
        "\n",
        "# Print the matching results (stemmed words and their frequency)\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cm584qoeFlz",
        "outputId": "3cc6fa89-c314-4b6e-f625-8d06ab158c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'birds wings'\n",
        "results = search_engine(url, query)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E3z42zqeZSQ",
        "outputId": "d4b6e768-228c-463b-de68-994976ade6b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query, index):\n",
        "    \"\"\"\n",
        "    Search for stemmed query words in the given index.\n",
        "\n",
        "    Parameters:\n",
        "    query (str): The search query string entered by the user.\n",
        "    index (dict): A dictionary mapping stemmed words to their frequency counts.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary containing the stemmed query words found in the index\n",
        "          along with their corresponding counts.\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    query_words = re.findall(r'\\w+', query.lower())\n",
        "    results = {}\n",
        "    for word in query_words:\n",
        "        word = stemmer.stem(word)\n",
        "        if word in index:\n",
        "            results[word] = index[word]\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "lK_fHt8Aebwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'birds wings'\n",
        "results = search_engine(url, query)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUBSB66JfHQM",
        "outputId": "5849a293-0fcc-4b9c-f23e-cd6401262ae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bird': 574, 'wing': 25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rank = 1  # Initialize rank to 1\n",
        "\n",
        "# Iterate over each word and its count in the results dictionary\n",
        "for word, count in results.items():\n",
        "    rank = rank * (1 / count)  # Multiply rank by the reciprocal of the count\n",
        "\n",
        "rank = 1 - rank  # Subtract the product from 1 to get the final rank value\n"
      ],
      "metadata": {
        "id": "izmK0W0TfT2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'birds wings'\n",
        "results = search_engine(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        "   rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)"
      ],
      "metadata": {
        "id": "OrF-H-FCfcB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'collage students'\n",
        "results = search_engine(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        " rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)\n"
      ],
      "metadata": {
        "id": "5gCceVI1fhVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'owls'\n",
        "results = search_engine(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        " rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)"
      ],
      "metadata": {
        "id": "1NGHh_Trfz1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Bird'\n",
        "query = 'Industry'\n",
        "results = search_engine(url, query)\n",
        "print(results)\n",
        "rank=1\n",
        "for word, count in results.items():\n",
        " rank = rank*1/count\n",
        "rank = 1-rank\n",
        "print(rank)"
      ],
      "metadata": {
        "id": "3AzhTcwKnuZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "### **מנוע המיועד למספר דפים**"
      ],
      "metadata": {
        "id": "NNHQfDngf-QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from collections import defaultdict\n",
        "class WikiSearchEngine:\n",
        "  def __init__(self):\n",
        "    \"\"\"Initialize the search engine\"\"\"\n",
        "    self.base_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    self.pages = []\n",
        "    self.word_locations = defaultdict(list) # word -> [(page_id, frequency), ...]\n",
        "    self.stop_words = {'a', 'an', 'the', 'and', 'or', 'in', 'on', 'at', 'to', 'for', 'of', 'with'}\n",
        "    return False\n",
        "  def fetch_wiki_pages(self, topic, num_pages=5):\n",
        "    \"\"\"Fetch Wikipedia pages for given topic\"\"\"\n",
        "    search_params = {\n",
        "      \"action\": \"query\",\n",
        "      \"format\": \"json\",\n",
        "      \"list\": \"search\",\n",
        "      \"srsearch\": topic,\n",
        "      \"srlimit\": num_pages\n",
        "    }\n",
        "    try:\n",
        "      response = requests.get(self.base_url, params=search_params)\n",
        "      search_results = response.json()['query']['search']\n",
        "\n",
        "      for result in search_results:\n",
        "        content_params = {\n",
        "            \"action\": \"query\",\n",
        "            \"format\": \"json\",\n",
        "            \"prop\": \"extracts|info\",\n",
        "            \"pageids\": result['pageid'],\n",
        "            \"inprop\": \"url\",\n",
        "            \"explaintext\": True\n",
        "        }\n",
        "        content_response = requests.get(self.base_url, params=content_params)\n",
        "        page_data = content_response.json()['query']['pages'][str(result['pageid'])]\n",
        "        self.pages.append({\n",
        "          'id': result['pageid'],\n",
        "          'title': page_data['title'],\n",
        "          'url': page_data.get('fullurl', f\"https://en.wikipedia.org/?curid={result['pageid']}\"),\n",
        "          'content': page_data['extract']\n",
        "        })\n",
        "      print(f\"Retrieved: {page_data['title']}\")\n",
        "      return True\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error fetching pages: {str(e)}\")\n",
        "\n",
        "  def build_index(self):\n",
        "        \"\"\"Build a simple word location index\"\"\"\n",
        "        self.word_locations.clear()\n",
        "\n",
        "        # Process each page\n",
        "        for page in self.pages:\n",
        "            # Get all words from content\n",
        "            words = re.findall(r'\\w+', page['content'].lower())\n",
        "\n",
        "            # Count word frequencies\n",
        "            word_counts = defaultdict(int)\n",
        "            for word in words:\n",
        "                if word not in self.stop_words:\n",
        "                    word_counts[word] += 1\n",
        "\n",
        "            # Add to index with page information\n",
        "            for word, count in word_counts.items():\n",
        "                self.word_locations[word].append((page['id'], count))\n",
        "\n",
        "  def search(self, query, num_results=5):\n",
        "        \"\"\"Search pages using simple word frequency ranking.\n",
        "        Ranks pages based on:1. Number of query words found in the page\n",
        "        2. Total frequency of query words  \"\"\"\n",
        "        # Get query words\n",
        "        query_words = [word.lower() for word in re.findall(r'\\w+', query)\n",
        "                    if word.lower() not in self.stop_words]\n",
        "        if not query_words:\n",
        "            return []\n",
        "\n",
        "        # Calculate scores for each page\n",
        "        page_scores = defaultdict(lambda: {'matches': 0, 'total_freq': 0})\n",
        "\n",
        "        # For each query word\n",
        "        for word in query_words:\n",
        "            # Find pages containing this word\n",
        "            for page_id, freq in self.word_locations.get(word, []):\n",
        "                page_scores[page_id]['matches'] += 1\n",
        "                page_scores[page_id]['total_freq'] += freq\n",
        "\n",
        "\n",
        "        # Convert to list and sort\n",
        "        ranked_results = [\n",
        "            (page_id, scores['matches'], scores['total_freq'])\n",
        "            for page_id, scores in page_scores.items()\n",
        "        ]\n",
        "        # Sort by number of matching words first, then by total frequency\n",
        "        ranked_results.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
        "        # Format results\n",
        "        results = []\n",
        "        for page_id, matches, total_freq in ranked_results[:num_results]:\n",
        "            page = next(p for p in self.pages if p['id'] == page_id)\n",
        "            # Find the first matching word context\n",
        "            context = self.get_context(page['content'], query_words)\n",
        "            results.append({\n",
        "                'title': page['title'],\n",
        "                'url': page['url'],\n",
        "                'matching_words': matches,\n",
        "                'total_frequency': total_freq,\n",
        "                'context': context\n",
        "            })\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "gK6qrTDSf91I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_wiki_pages(self, topic, num_pages=5):\n",
        "  \"\"\"Fetch Wikipedia pages for given topic\"\"\"\n",
        "  search_params = {\n",
        "  \"action\": \"query\",\n",
        "  \"format\": \"json\",\n",
        "  \"list\": \"search\",\n",
        "  \"srsearch\": topic,\n",
        "  \"srlimit\": num_pages\n",
        "  }\n",
        "  try:\n",
        "      response = requests.get(self.base_url, params=search_params)\n",
        "      search_results = response.json()['query']['search']\n",
        "\n",
        "      for result in search_results:\n",
        "        content_params = {\n",
        "            \"action\": \"query\",\n",
        "            \"format\": \"json\",\n",
        "            \"prop\": \"extracts|info\",\n",
        "            \"pageids\": result['pageid'],\n",
        "            \"inprop\": \"url\",\n",
        "            \"explaintext\": True\n",
        "        }\n",
        "        content_response = requests.get(self.base_url, params=content_params)\n",
        "        page_data = content_response.json()['query']['pages'][str(result['pageid'])]\n",
        "        self.pages.append({\n",
        "          'id': result['pageid'],\n",
        "          'title': page_data['title'],\n",
        "          'url': page_data.get('fullurl', f\"https://en.wikipedia.org/?curid={result['pageid']}\"),\n",
        "          'content': page_data['extract']\n",
        "        })\n",
        "        print(f\"Retrieved: {page_data['title']}\")\n",
        "      return True\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error fetching pages: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "XvdwliQ7hEhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def build_index(self):\n",
        "        \"\"\"Build a simple word location index\"\"\"\n",
        "        self.word_locations.clear()\n",
        "\n",
        "        # Process each page\n",
        "        for page in self.pages:\n",
        "            # Get all words from content\n",
        "            words = re.findall(r'\\w+', page['content'].lower())\n",
        "\n",
        "            # Count word frequencies\n",
        "            word_counts = defaultdict(int)\n",
        "            for word in words:\n",
        "                if word not in self.stop_words:\n",
        "                    word_counts[word] += 1\n",
        "\n",
        "            # Add to index with page information\n",
        "            for word, count in word_counts.items():\n",
        "                self.word_locations[word].append((page['id'], count))\n",
        "\n",
        "    def search(self, query, num_results=5):\n",
        "        \"\"\"Search pages using simple word frequency ranking.\n",
        "        Ranks pages based on:1. Number of query words found in the page\n",
        "        2. Total frequency of query words  \"\"\"\n",
        "        # Get query words\n",
        "        query_words = [word.lower() for word in re.findall(r'\\w+', query)\n",
        "                      if word.lower() not in self.stop_words]\n",
        "        if not query_words:\n",
        "            return []\n",
        "\n",
        "        # Calculate scores for each page\n",
        "        page_scores = defaultdict(lambda: {'matches': 0, 'total_freq': 0})\n",
        "\n",
        "        # For each query word\n",
        "        for word in query_words:\n",
        "            # Find pages containing this word\n",
        "            for page_id, freq in self.word_locations.get(word, []):\n",
        "                page_scores[page_id]['matches'] += 1\n",
        "                page_scores[page_id]['total_freq'] += freq\n",
        "\n",
        "\n",
        "        # Convert to list and sort\n",
        "        ranked_results = [\n",
        "            (page_id, scores['matches'], scores['total_freq'])\n",
        "            for page_id, scores in page_scores.items()\n",
        "        ]\n",
        "        # Sort by number of matching words first, then by total frequency\n",
        "        ranked_results.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
        "        # Format results\n",
        "        results = []\n",
        "        for page_id, matches, total_freq in ranked_results[:num_results]:\n",
        "            page = next(p for p in self.pages if p['id'] == page_id)\n",
        "            # Find the first matching word context\n",
        "            context = self.get_context(page['content'], query_words)\n",
        "            results.append({\n",
        "                'title': page['title'],\n",
        "                'url': page['url'],\n",
        "                'matching_words': matches,\n",
        "                'total_frequency': total_freq,\n",
        "                'context': context\n",
        "            })\n",
        "        return results"
      ],
      "metadata": {
        "id": "F-5SwSt6jv3T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}